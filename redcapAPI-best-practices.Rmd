---
title: "redcapAPI Best Practices"
author: "Benjamin Nutter, Savannah Obregon, Shawn Garbett"
date: "`r Sys.Date()`"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Introduction

[REsearch Data Collection or REDCap](https://projectredcap.org) puts a lot
of power into the hands of folks wishing to collect data, from surveys to 
running clinical trials. Once the data is collected the task of summarization 
into reports falls upon the statistician or data scientist.
[R](https://www.r-project.org/) being a useful tool, the department of
Biostatistics at Vanderbilt University Medical Center has provided the
community with [`redcapAPI`](https://cran.r-project.org/package=redcapAPI) to
facilitate using REDCap from R. 

`redcapAPI` has grown a lot over time, and recently what
once worked inside the code and interface for `redcapAPI` no longer aligned
with where the REDCap project was today. To this end, a major refactor
based on user feedback was undertaken to better address the challenges
of a researcher in todays computing environments. This new interface
began with version 2.7.0.

Primarily the means of retrieving records has changed from `exportRecords` to
`exportRecordsTyped`. The rename of the function is due to the change of the
interface to allow time for systems to switch over to the new function. It
is important to read over this document and understand the changes if one
is a current user of `exportRecords`. However, the changes go much deeper
and this document will outline what is a best practices approach to using
the library.

The real goal is that a user make the fewest calls to do accomplish their
job and have data ready for analysis. This can't happen without user 
involvement--if the library doesn't work easily for your needs open an
issue on [github](https://github.com/vubiostat/redcapAPI/issues) and 
we'll do our best to work with you. 

If one wishes to reproduce these examples, see '*Reproducing this Vignette*' towards the end of this document. 

## API_KEY security

The first thing to consider is the API_KEY. This key is what allows for data
export from a REDCap project. It is the equivalent of a user name, password and
project identifier in a single character string. As such it should be protected
as strongly as your password into the systems storing ones data. In the
United States, the HIPAA law has a minimum violation of $100 per private health
record exposed. In a large clinical trial setting this can easily run into
millions of dollars of potential risk.

As such, **the API_KEY should never be stored in a plain text file** unless
it's on a tightly monitored and secured production system that cannot work
without it.

Logging into REDCap every time one wants to work, and then having to juggle
multiple API_KEYs quickly becomes burdensome. Copy and pasting that API_KEY
into code (plain text!) and then remembering to delete when finished is too
easy to forget. A single git commit and simple push to share code and
the API_KEY is exposed to the world. It's a very easy mistake to make. In 
the United States there is a minimum HIPAA penalty of $100 per medical record 
exposed. An API_KEY that accesses a large clinical trial, if published
could resulting millions of dollars in fines. 

What follows is an example helper function using an encrypted local file
to store API_KEYS to open connections to REDCap. Using this function
greatly reduces the risk of exposure. 

Note: *This functionality was originally in the package `rccola`, but this
library is no longer needed. The functionality is built into `redcapAPI` and
only requesting connections is supported. This is the preferred long term 
solution.*


```{r unlock}
library(redcapAPI)
library(Hmisc)

# Cuts down on password requests on MAC
options(keyring_backend=keyring::backend_file)

unlockREDCap(c(test_conn    = 'TestRedcapAPI', # REDCap project 1
               sandbox_conn = 'SandboxAPI'),   # REDCap project 2
             keyring      = 'API_KEYs',
             envir        = globalenv(),
             url          = 'https://redcap.vanderbilt.edu/api/') 
```

The first time this is called, it asks the user for a password that will be used
to unlock the crypto locker `API_KEYs`. A keyring can contain multiple API_KEYs
and hence the name we've given it here--one is free to use any naming they
desire. The first time it is run it will prompt for each
API_KEY by the name you've given it, e.g. 'TestRedcapAPI'. If an API_KEY
does not connect the call will fail and halt execution in R and it will be
deleted from the key_ring to prompt you to enter it again. Subsequent 
calls will not prompt for an API_KEY, just the password you've given
to unlock the remaining keys in the locker. It will stay open in an R session
and not prompt again (*caveat: each knitr call creates a new session, so it
will prompt each call to knitr*). 

Specifying `envir=globalenv()` tells the function to create the connections
in the global environment as variables. Without this the function returns 
a list of the connections. 

In summary, the keyring is stored in an 
encrypted form accessible by a single password. If ones laptop were stolen or
compromised it is far more difficult for a hacker to gain further access due
to the encryption. 

This library also cooperates with our production environments by looking for
these things in an plain text file `yml` in the directory above execution. This
functionality is *only* recommended for system admins and should **never** be
used on a work desktop or laptop.

Also to make sure that R in trying to be helpful doesn't save API_KEYs to 
files, it is recommended to turn off any saving of workspace data. In RStudio
this is under "Tools -> Global Options (General)", set 'Save Workspace to .RData on 
exit' to *Never*.

For command line users create an `.Rprofile` file in your home directory 
containing the following base function override:

```
utils::assignInNamespace(
  "q",
  function(save="no", status=0, runLast=TRUE)
  {
    .Internal(quit(save, status, runLast))
  },
  "base"
)
```

If the easiest path is the best path, it will become the common path. We've
done our best to make best security practice the easiest path. 

## The Connection Object (caching)

The connection objects are a much richer object. During a lot of REDCap 
interaction the meta data is necessary to properly interpret the data and
guides data transformation. Instead of calling multiple times with each
call for this data, the meta data is now cached in the connection object.

Caching saves a lot of round trip calls, but brings with it the burden that
sometimes it needs to be refreshed. For example, one is developing in a 
REDCap object and has an R environment interacting with it. After a call, 
it's noted that something needs changed in the project proper. Using the REDCap
GUI the project's definition is changed. This requires flushing the cache
so the next call will retrieve and cached the new data. 

```{r flushcache}
head(test_conn$fieldnames())
test_conn$flush_fieldnames()

head(test_conn$metadata())
test_conn$flush_metadata()

test_conn$flush_all()

```

Tip: Remember to flush cache after updating project meta data in the GUI.

Another benefit the new connection object brings is the idea of retry. When 
developing, it's okay if the network hiccups, one can simple rerun the report
or command and try again. In a production environment, a report that makes
a lot of API calls is assuming that all of those calls are successful to be
successful. This is not that case 100% of the time, so a mitigation strategy
is needed on the connection object. This is implemented via the `retries`,
`retry_interval` and `retry_quietly` parameters when calling to build the
connection objects. These are passed to `redcapAPI::redcapConnection` as
additional parameters.
The default is to quietly make 5 retries on a call, with an interval of 2, 4, 8,
16, and 32 seconds between retries. This greatly improves the odds of
building a complex report involving a lot of REDCap calls. The user
of the package gets this for **free**. 

## `exportRecordsTyped`

`exportRecords`, `redcapFactor` and `redcapFlipFactor` still exist in the
library but are deprecated. `exportRecordsTyped` is the preferred way forward.

Now armed with a connection from a secured API_KEY in one's R session, the 
usual goal is to get the data into R, properly typed for use in an R
model. Dates and Factors need to be converted into a usable format that makes
statistical modelling easy. Type theory is a very deep theoretical topic
in mathematics and computer science and thus this topic is a complex.
`redcapAPI` has made a lot of default choices which we felt will satisfy 80% of
use cases.

However, these choices are not a limitation. Care has been taken to allow
each of these choices a user defined override and be extensible to handle
just about anything the user would prefer. The strategy chosen is
called _inversion of control_. 

Understanding the type 'casting' algorithm is important if the default
choices are not satisfactory. Casting refering to the transformation
of one data class in R to another (aka type casting). 

### The algorithm

REDCap stores all data as character strings. A validation on input may be
specified as a `field_type` in the REDCap project. However, these might
be added later, changed or raw data from a different system pushed up. The
declared `field_type` from the REDCap meta data has no guarantee to describe
the data format of the actual data. This divergence can be a source of
frustration and difficulty, thus we've designed the following steps of
the process to cast a column of data from a project:

1. Detect fields that are NA. This defaults to `""` or `"NA"`.
2. Fields that are not NA, are passed through a validation for the `field_type`.
3. Fields that are not NA, that pass validation are then cast to the desired class.

The choice of which routine to call is a defined by `field_type`. The current
version of REDCap as of this writing is: `date_`, `datetime_`,
`datetime_seconds_`, `time_mm_ss`, `time_hh_mm_ss`, `time`, `float`, `number`,
`calc`, `int`, `integer`, `yesno`, `truefalse`, `checkbox`, `form_complete`,
`select`, `radio`, `dropdown`, and `sql`.

The field_type for `date_`, `datetime_` and `datetime_seconds` are all 
truncated from the original as all of these are reported in the API as ymd. 

### NA

The definition of NA may vary. An example is someone uploaded external data
that says "-5" is an NA due to a code book. These values are not desired
to be treated as anything but NA. In this case the user needs to specify
an override.

The expected function signature is `function(x, field_name, coding)`. The
following demonstrates some test data. It follows with a declaration that date 
"2023-03-24" is to be treated as NA. Then, "2023-03-24" is only to be treated
as NA for the field `date_mdy`.  Coding is only provided if there is a
defined code book for the variable. 

```{r na_exp}
head(exportRecordsTyped(test_conn)[,1:10])

my_na_detector <- function(x, field_name, coding) is.na(x) | x=="" | x == "2023-02-24"

head(exportRecordsTyped(test_conn, na=list(date_=my_na_detector))[,1:10])

my_limited_na_detector <- function(x, field_name, coding)
  is.na(x) |
  x==""    |
  field_name=='date_mdy'
                                   
head(exportRecordsTyped(test_conn, na=list(date_=my_limited_na_detector))[,1:10])

```

It is hopefully a rare case when this is needed. The next step, validation, has
an available report that will hopefully make it clear when it is. 

### Validation

This step based on `field_type` calls a function that returns a vector of 
logical specifying what is valid or not. The simplest of these is via
a regular expression or regex. Detailing construction of a regex for
validation of a field is outside the scope of this document, good tutorials
are available online such as
[https://regextutorial.org/](https://regextutorial.org/). It's helpful to have
an interactive environment to develop one, we used 
[https://regex101.com/](https://regex101.com/) frequently in developing the 
regexs provided by default.

The function signature once again is `function(x, field_name, coding)`. 

The default set of validations is:

```
list(
  date_              = valRx("^[0-9]{1,4}-(0?[1-9]|1[012])-(0?[1-9]|[12][0-9]|3[01])$"),
  datetime_          = valRx("^[0-9]{1,4}-(0?[1-9]|1[012])-(0?[1-9]|[12][0-9]|3[01])\\s([0-9]|0[0-9]|1[0-9]|2[0-3]):[0-5][0-9]$"),
  datetime_seconds_  = valRx("^[0-9]{1,4}-(0?[1-9]|1[012])-(0?[1-9]|[12][0-9]|3[01])\\s([0-9]|0[0-9]|1[0-9]|2[0-3]):[0-5][0-9]:[0-5][0-9]$"),
  time_mm_ss         = valRx("^[0-5][0-9]:[0-5][0-9]$"),
  time_hh_mm_ss      = valRx("^([0-9]|0[0-9]|1[0-9]|2[0-3]):[0-5][0-9]:[0-5][0-9]$"),
  time               = valRx("^([0-9]|0[0-9]|1[0-9]|2[0-3]):[0-5][0-9]$"),
  float              = valRx("^[-+]?(([0-9]+\\.?[0-9]*)|(\\.[0-9]+))([Ee][+-]?[0-9]+)?$"),
  number             = valRx("^[-+]?(([0-9]+\\.?[0-9]*)|(\\.[0-9]+))([Ee][+-]?[0-9]+)?$"),
  calc               = valRx("^[-+]?(([0-9]+\\.?[0-9]*)|(\\.[0-9]+))([Ee][+-]?[0-9]+)?$"),
  int                = valRx("^[-+]?[0-9]+(|\\.|\\.[0]+)$"),
  integer            = valRx("^[-+]?[0-9]+$"),
  yesno              = valRx("^(?i)(0|1|yes|no)$"),
  truefalse          = valRx("^(0|1|true|false)$"),
  checkbox           = valRx("^(?i)(0|1|yes|no)$"),
  form_complete      = valRx("^[012]$"),
  select             = valChoice,
  radio              = valChoice,
  dropdown           = valChoice,
  sql                = NA # Incomplete at present
)
```

Ignore the complex regular expressions above if you're not familiar. Let's look
at a building a simple validation for `form_complete`: `valRx("^[012]$")`. The
regex here starts with "^" for beginning of string, it's followed by a set
in square brakets meaning to match one of those characters, then the "$" meaning
end of string. Thus, it asks to build a validation function of the right
signature that will return a vector that is TRUE for input that is a single
character "0", "1" or "2" and FALSE otherwise. 

All characters that fail a validation are returned as an attribute "invalid"
on the resulting data.frame. The default print method will format this
into Markdown, and all records that are not NA that fail validation will be
called out. 

We will use a RegEx to make a lot of numbers fail in this example, and use
the [1:10,] selector to limit the output for this example.

```{r validation}
Records <- exportRecordsTyped(test_conn,
             validation=list(number=valRx("^5$|^-100$")))
summary(Records$prereq_number)
knitr::asis_output(format(attr(Records, "invalid")[1:10,]))
```

This shows that the number records containing "1" did not pass the regex validation
and these will become NA in the final output. The field name, type, row number and record id all help the user to quickly diagnose what is not validating.

Once again, overriding the default is expected to be a rare need, but the
option is available should it arise. Casting variables to the desired class
is up next. 

### Casting

The na and validation callback list serve to exclude what should not be
attempted to cast into a class. This prevents the library from crashing
when the input does not match the expected format. This is particularly
troublesome with date and time casting, and excluding these failed validations
ensures the cast will be successful. 

The function signature for these callbacks is the familiar `function(x, field_name, coding)`.


```
list(
  date_              = function(x, ...) as.POSIXct(x, format = "%Y-%m-%d"),
  datetime_          = function(x, ...) as.POSIXct(x, format = "%Y-%m-%d %H:%M"),
  datetime_seconds_  = function(x, ...) as.POSIXct(x, format = "%Y-%m-%d %H:%M:%S"),
  time_mm_ss         = function(x, ...) chron::times(ifelse(is.na(x),NA,paste0("00:",x)), format=c(times="h:m:s")),
  time_hh_mm_ss      = function(x, ...) chron::times(x, format=c(times="h:m:s")),
  time               = function(x, ...) chron::times(gsub("(^\\d{2}:\\d{2}$)", "\\1:00", x), 
                                                     format=c(times="h:m:s")),
  float              = as.numeric,
  number             = as.numeric,
  calc               = as.numeric,
  int                = as.integer,
  integer            = as.numeric,
  yesno              = castLabel,
  truefalse          = function(x, ...) x=='1' | tolower(x) =='true',
  checkbox           = castChecked,
  form_complete      = castLabel,
  select             = castLabel,
  radio              = castLabel,
  dropdown           = castLabel,
  sql                = NA
)
```

A common request is instead of using POSIXct for the dates, to use the
internal as.Date function. 

NOTE: An exported object `cast_raw` consists of NA for each of these keys. If 
one desires raw data the cast function is NA. 

```{r date_cast}
head(exportRecordsTyped(test_conn, cast=list(date_=as.Date))[,1:10])

```
The date columns are now of the internal base R `date` class. Various helper
routines are available on the `?fieldValidationAndCasting` help page. One 
of note is `castCode` which when used instead of `castLabel` it will cast
to the coded value and not the labelled value. 

With na, validation and cast covered a large amount of new functionality 
and control is in the hands of the user.

### Labels and Units

Inversion of control is available for the assignment of attributes to columns
as well. There exists an assignment argument which will is a list of functions
that will assign their output to the attribute using the name of the list key.

The defaults add labels and units. 

```
assignment=list(label=stripHTMLandUnicode, units=unitsFieldAnnotation)
```
The function signature for these is `function(field_name, field_label, field_annotation)`. 

The label for a column is created by stripping HTML and Unicode characters from
the REDCap field label. The units are done by searching the field annotation
for something of the following form: `units={"meters"}` (using a regex). 

If one desired custom attributes on columns based on this information it can 
be done with an override.

### Forms

If the forms argument is specified, the return from `exportRecordsTyped` filters
the data down to only rows and columns containing information for the specified
forms. 

```{r}
exportRecordsTyped(test_conn, forms="repeating_instrument")
```

There are only 2 rows from a test project with over 20 rows of data. 

## Post Processing

The scope and purpose of `exportRecordsTyped` was to extract the data frame in
the desired classes for analysis. Sometimes post processing of the frame
for further cleanup is desired and casting cannot do all that is required.
Several useful helper routines for post processing are provided. 
The first we'll cover is `recastRecords`. 

### `recastRecords`

User have reported that `redcapFactorFlip` has been very useful for them
to switch the way the data was cast in a back and forth manner. The current
library has deprecated `redcapFactorFlip` and the new method to replace it
is `recastRecords`.

```{r recast}
exportRecordsTyped(test_conn,
  fields=c("record_id", "date_dmy",
           "date_mdy", "prereq_yesno")) |>
  recastRecords(test_conn,
                fields = c("date_dmy", "date_mdy", "prereq_yesno"),
                cast   = list(date_  = as.Date,
                              yesno = castRaw)) |>
head()
```

Recasting may be performed using a character vector of field names; a numeric 
vector of field indices; or a logical vector (the logical vector must be
the same length as the number of columns in the data frame).

### mChoice

Users of `Hmisc` or `rms` might want multiple choice class fields added to 
their resulting Record data.frame. 

```{r mchoice}
x <- exportRecordsTyped(test_conn) |> mChoiceCast(test_conn)
x$checkbox_test
```

### guessCast

What if validations were never added to the project and one would like to
take a guess at casting, i.e. not rely on the meta data? Any field that
remains character can be subject to a guess based on passing validation.

This is kept as a separate function to ensure that the user makes 
a clear choice in using guesswork. 

```{r guess}
exportRecordsTyped(test_conn, fields="date_dmy", cast=raw_cast) |>
  guessCast(
    test_conn,
    validation=valRx("^[0-9]{1,4}-(0?[1-9]|1[012])-(0?[1-9]|[12][0-9]|3[01])$"), 
    cast=as.Date,
    threshold=0.1)
```

Since dates are common, a helper specifically for this guess is provided.

```{r guessDate}
exportRecordsTyped(test_conn, fields="date_dmy", cast=raw_cast) |>
  guessDate(test_conn, threshold=0.1)
```

### Widen / Shorten a Repeating Instrument

FIXME: Savannah--need a good writeup here.

### castForImport

While it is true that `importRecords` will convert most data types into a 
format that can be imported, it has proven to be overly rigid with blind 
spots that cannot be easily overcome. In order to provide better support for
importing data, we have provided the stand alone function `castForImport`.

`castForImport` follows the same strategy of validation and casting 
used in `exportRecordsTyped`. It returns a data frame where the fields are 
cast in a format (usually character) that can be passed into `importRecords`. 

```{r}
Records <- exportRecordsTyped(test_conn, 
                              records = 10:29, 
                              forms = "data_import_export")

Records$checkbox_test___x

Records$dropdown_test
```

The default settings of `castForImport` are arranged so that most cases of 
data will be recast for import as desired. 

```{r}
ForImport <- castForImport(Records, 
                           test_conn)

ForImport$checkbox_test___x

ForImport$dropdown_test
```

The actual default casting list for `castForImport` is 

```{r, eval = FALSE}
list(
  date_                    = as.character,
  datetime_                = as.character,
  datetime_seconds_        = as.character,
  time_mm_ss               = castTimeMMSS,
  time_hh_mm_ss            = as.character,
  time                     = castTimeHHMM,
  alpha_only               = as.character,
  float                    = as.character,
  number                   = as.character,
  number_1dp               = castDpCharacter(1, dec_symbol = "."), 
  number_1dp_comma_decimal = castDpCharacter(1),
  number_2dp               = castDpCharacter(2, dec_symbol = "."), 
  number_2dp_comma_decimal = castDpCharacter(2),
  calc                     = as.character,
  int                      = function(x, ...) as.character(as.integer(x)),
  integer                  = function(x, ...) as.character(as.integer(x)),
  yesno                    = castRaw,
  truefalse                = function(x, ...) (x=='1' | tolower(x) =='true') + 0L,
  checkbox                 = castRaw,
  form_complete            = castRaw,
  select                   = castRaw,
  radio                    = castRaw,
  dropdown                 = castRaw,
  email                    = as.character, 
  phone                    = as.character,
  zipcode                  = as.character, 
  slider                   = as.numeric,
  sql                      = NA
)
```

At this time, we have not changed anything within `importRecords`. Doing so 
would require making a breaking change and we aren't prepared to do that on 
short notice. However, we are considering this change in the future. For that 
reason, we advise changing your processes to utilize `castForImport` prior
to importing data. 

#### Customizing Checkbox Casting

We have encountered a special case of a checkbox function that `importRecords` 
is entirely incapable of handling. In this case, the checkbox variable was
defined with the options

```
0, 0
1, 1
2, 2
```

In this case, the field `checkbox_example___0` could actually be cast in a way
that "0" indicates the checkbox was "checked". This is a scenario that is 
problematic, as the API would determine that any value of "0" is unchecked. 

In order to handle this scenario, we have provided a special casting function, 
`castCheckForImport`, that allows the user to designate what values are to 
represent a checked value. 

```{r}
Records <- data.frame(checkbox_test___x = c("0", "", "", "0"), 
                      checkbox_test___y = c("y", "y", "", "y"))

ForImport <- castForImport(Records, 
                           test_conn, 
                           fields = "checkbox_test___x",
                           cast = list(checkbox = castCheckForImport(checked = "0")))
ForImport
```

To complete an import in this scenario may require two passes with `castForImport`
before calling `importRecords`.

```{r, eval = FALSE}
ForImport <- 
  castForImport(Records, 
                test_cond, 
                fields = "checkbox_test___x", 
                cast = list(checkbox = castCheckForImport(checked = "0"))) |>
  castForImport(test_conn)

importRecords(test_conn, 
              data = ForImport)
```
## Even More

### Bulk Records Pull

A helper function is provided that will pull a set of projects and/or forms
from projects into memory and apply a post processor set in bulk. 

```{r}
exportBulkRecords(
 rcon  = list(test = test_conn,
              sand = sandbox_conn),
 forms = list(test = c('repeating_instrument', 'branching_logic')),
 envir = globalenv(),
 post  = function(Records, rcon)
         {
           Records              |>
           mChoiceCast(rcon)    |>
           guessDate(rcon)      #|>
           #widerRepeating(rcon)
         }
)
test_repeating_instrument
test_branching_logic
```

This references the connections we opened in the `unlockREDCap` section at
the beginning and provides the names we want for the resulting records.
The environment post execution contains the data.frames: `test.repeating_instrument`,
`test.branching_logic`, `sand`. Each of these were retrieved, possibly using the forms
argument and all were post processed in the same manner as specified by `post`.
Any additional arguments are passed on to the `exportRecordsTyped` call. 

### Branching Logic NA detection

The `missingSummary` function provides a utility to look for missing values 
within a dataset. The results account for branching logic in the instrument; 
fields that are missing because branching logic did not expose them do not
get counted as missing. One may restrict the summary to fields, forms, and/or 
records as well. 

```{r}
missingSummary(test_conn, 
               exportRecordsArgs = list(records = 10:29, 
                                        fields = "record_id", 
                                        forms = "branching_logic")) |>
  head()
```

One limitation of `missingSummary`, however, is that the summary operates 
exclusively within each row of the data. Thus, if your branching logic 
utilizes values from previous events, this summary will not correctly identify
non-missing values. 

### Design your own API Calls

`redcapAPI` are very specific in how they access the REDCap API and leave 
very little flexibility to the user in the choice of arguments to pass to the
API. This lack of flexibility is deliberate, as it helps limit the potential
for errors and frustration to the typical user. Advanced users may, at times, 
find our decisions limiting. Of there may be a need to use an API method that
`redcapAPI` does not yet offer. 

Users wishing to customize their API calls may use `makeApiCall`. This is a 
flexible function that utilizes the `redcapConnection` object, permitting 
customized calls within the same code style of the rest of the package. It has an added benefit of the retry strategy for API call failures as mentioned in the '*Connection*' section above.

For example, using `makeApiCall`, we can get User-Roles, even though `redcapAPI`
does not have a dedicated method to retrieve those.

```{r}
response <- makeApiCall(sandbox_conn, 
                        body = list(content = 'userRole', 
                                    format = 'csv', 
                                    returnFormat = 'csv'))
read.csv(text = as.character(response), 
         na.strings = '')
```


### Cornacopia of Functions to explore

The functions offered by `redcapAPI` have expanded significantly in recent
versions. The table below names all of the methods provided by the REDCap
API and indicates which are supported by `redcapAPI`. 

| System          | Export | Import | Delete | Other Method | 
|---------|-----|-----|-----|--------------|
| Arms            | Yes    | Yes    | Yes    | N/A          |
| DAGs            | No     | No     | No     | SwitchDag (No)   | 
|                 |        |        |        | exportDagAssigment (No) | 
|                 |        |        |        | importDagAssigment (No) |
| Events          | Yes    | Yes    | Yes    | N/A                |
| Field Names     | Yes    | N/A    | N/A    | N/A                |
| Files           | Yes    | Yes    | Yes    | N/A                |
| File Repository | Yes    | Yes    | Yes    | createFileRepositoryFolder (Yes) |
|                 |        |        |        | exportFileRepositoryListing (Yes) |
| Instruments     | Yes    | N/A    | N/A    | exportMappings (Yes) | 
|                 |        |        |        | importMappings (Yes) | 
| Logging         | Yes    | N/A    | N/A    | N/A                  |
| Meta Data       | Yes    | Yes    | N/A    | N/A                  | 
| Project Info    | Yes    | Yes    | N/A    | createProject (No)   |
|                 |        |        |        | exportProjectXML (No) | 
| Records         | Yes    | Yes    | Yes    | renameRecord (No)    | 
|                 |        |        |        | exportNextRecordName (Yes) | 
| Reports         | Yes    | N/A    | N/A    | N/A                  |
| Version         | Yes    | N/A    | N/A    | N/A                  |
| Surveys         | N/A    | N/A    | N/A    | exportSurveyLink (No) | 
|                 |        |        |        | exportSurveyParticipants (Yes) | 
|                 |        |        |        | exportSurveyQueueLink (No) | 
|                 |        |        |        | exportSurveyReturnCode (No) | 
| Users           | Yes    | No     | No     | N/A                   |
| UserRoles       | No     | No     | No     | exportUserRoleAssigments (No) | 
|                 |        |        |        | importUserRoleAssigments (No) |


### Reproducing this Vignette

This requires building one's own test database. Functions to build a REDCap project and load with data are included, as well as the
test database this was built from 

FIXME: Thomas Dupont - Example building test database here. Pull from existing tests.

## Thanks

Thanks to all those that have made this effort possible for `redcapAPI` as an R package, and striven to make it better.

* Cole Beck
* Lynne Berry
* Caroline Birdrow
* Thomas Dupont
* Shawn Garbett
* Will Gray
* Frank Harrell
* **Jeffrey Horner**
* Omair Khan
* Dandan Liu
* **Benjamin Nutter**
* Savannah Obregon
* Jeremy Stephens
* The Biostatistics department at Vanderbilt University Medical Center
* The R-Project and CRAN team
* The REDCap Team at Vanderbilt University Medical Center
* National Institutes of Health (NIH/NCATS UL1 TR000445)
